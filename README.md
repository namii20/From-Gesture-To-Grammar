**From Gesture to Grammar: A Deep Neural Approach to Sign Language Understanding**

**ğŸ“Œ Overview**

A computer vision & deep learning system for ASL recognition, translating static hand gestures (Aâ€“Y, excluding J & Z) into text. Supports real-time recognition via webcam to bridge communication between hearing-impaired and non-signers.

**ğŸ§  Key Contributions**

CNN-based model for static ASL gestures

Image preprocessing & augmentation applied

Achieved ~94% accuracy

Real-time webcam recognition

Extensions planned for dynamic gestures & full sign-to-text translation

**ğŸ—‚ï¸ Dataset**

ASL Alphabet Dataset | 29 classes (Aâ€“Z excluding J & Z, plus space, delete, nothing)

Preprocessing: Resize, normalize, background reduction, augmentation

Split: 70% train | 20% val | 10% test

Dataset Link: https://www.kaggle.com/datasets/avnijaiswal/asl-alphabet-dataset

**ğŸ—ï¸ Model**

CNN layers for spatial features

Optional LSTM for temporal patterns (future)

Input: 64Ã—64Ã—3 | Optimizer: Adam (lr=0.001) | Batch: 32 | Epochs: 30â€“50

Libraries: TensorFlow, Keras, OpenCV, NumPy, Pandas, Matplotlib

**ğŸ“Š Results**

Accuracy: ~94%

Most classes: strong precision, recall, F1

Minor confusion in visually similar gestures (A vs V)

Real-time testing: webcam recognition works well

**âš ï¸ Limitations**

Dynamic gestures (J & Z) not included

Limited testing across diverse users/environments

Performance may drop in poor lighting/clutter

**ğŸš€ Future Work**

Add dynamic gesture recognition (LSTM/Transformer)

Expand dataset with varied users & conditions

Transfer learning (ResNet, MobileNet, VGG)

Deploy as mobile/web app

Integrate sign-to-speech translation

ğŸ‘©â€ğŸ’» Authors

Avani Jaiswal â€“ B.Tech in AI & ML, Indira Gandhi Delhi Technical University for Women (IGDTUW)

Namita Belwal â€“ B.Tech in ECE-AI, Indira Gandhi Delhi Technical University for Women (IGDTUW)
